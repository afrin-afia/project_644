{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d27e0f-a482-4e93-ac77-8b7baf4f8067",
   "metadata": {},
   "source": [
    "# 2. Splitting a Pytorch dataset into unbalanced partitions\n",
    "Author: Javier Sales-Ortiz\n",
    "\n",
    "Date: Nov, 2022\n",
    "\n",
    "CMPUT 644 Final project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1da8252-8159-4311-ae13-06510bb4d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "\n",
    "import flwr as fl\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from flwr.common import Metrics\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import FashionMNIST \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "\n",
    "# For function\n",
    "from partition import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe32e3d-a40d-42c3-b4cd-772ed86caf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=30, linewidth=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9ee9cd-83da-434e-9f38-12ae67c6c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root dir for FashionMNIST\n",
    "root = '../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f526a9f-3bcf-42ed-b739-d124790a1a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "\n",
      "Type and dir of training_data\n",
      "<class 'torchvision.datasets.mnist.FashionMNIST'>\n",
      "\n",
      "training_data.train_data\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([60000, 28, 28])\n",
      "\n",
      "training_data.train_labels\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([60000])\n",
      "\n",
      "Type and dir of test_data\n",
      "<class 'torchvision.datasets.mnist.FashionMNIST'>\n",
      "['__add__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_check_exists', '_check_legacy_exist', '_format_transform_repr', '_is_protocol', '_load_data', '_load_legacy_data', '_repr_indent', 'class_to_idx', 'classes', 'data', 'download', 'extra_repr', 'mirrors', 'processed_folder', 'raw_folder', 'resources', 'root', 'target_transform', 'targets', 'test_data', 'test_file', 'test_labels', 'train', 'train_data', 'train_labels', 'training_file', 'transform', 'transforms']\n",
      "\n",
      "test_data.test_data\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([10000, 28, 28])\n",
      "\n",
      "test_data.test_labels\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([10000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/javier/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/javier/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/home/javier/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {DEVICE} device\")\n",
    "\n",
    "NUM_CLIENTS = 10\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "[transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# Load in the Fashion MNIST dataset\n",
    "training_data = FashionMNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(\"\\nType and dir of training_data\")\n",
    "print(type(training_data))\n",
    "#print(dir(training_data))\n",
    "\n",
    "print(\"\\ntraining_data.train_data\")\n",
    "print(type(training_data.train_data))\n",
    "print(training_data.train_data.shape)\n",
    "\n",
    "print(\"\\ntraining_data.train_labels\")\n",
    "print(type(training_data.train_labels))\n",
    "print(training_data.train_labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "test_data = FashionMNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(\"\\nType and dir of test_data\")\n",
    "print(type(test_data))\n",
    "print(dir(test_data))\n",
    "\n",
    "print(\"\\ntest_data.test_data\")\n",
    "print(type(test_data.test_data))\n",
    "print(test_data.test_data.shape)\n",
    "\n",
    "print(\"\\ntest_data.test_labels\")\n",
    "print(type(test_data.test_labels))\n",
    "print(test_data.test_labels.shape)\n",
    "\n",
    "# Split the training data into NUM_CLIENTS clients\n",
    "partition_size = len(training_data) // NUM_CLIENTS\n",
    "lengths = [partition_size] * NUM_CLIENTS\n",
    "datasets = random_split(training_data, lengths, generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec2deb-d02a-42f2-9ea6-80ef8875c59a",
   "metadata": {},
   "source": [
    "## Train data to numpy \n",
    "Do some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e270b06-0ae8-4e92-a920-ba61c364bc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(60000,)\n",
      "[0 1 2 3 4 5 6 7 8 9] [6000 6000 6000 6000 6000 6000 6000 6000 6000 6000]\n"
     ]
    }
   ],
   "source": [
    "np_train_labels = training_data.train_labels.cpu().detach().numpy()\n",
    "np_train_data = training_data.train_data.cpu().detach().numpy()\n",
    "\n",
    "print(type(np_train_labels))\n",
    "print(np_train_labels.shape)\n",
    "\n",
    "lunique, lcount = np.unique(np_train_labels, return_counts = True)\n",
    "print(lunique, lcount)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6536a5c-3c06-410f-ba9a-b3db51e45da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[0 1 2 3 4 5 6 7 8 9] [1000 1000 1000 1000 1000 1000 1000 1000 1000 1000]\n"
     ]
    }
   ],
   "source": [
    "np_test_labels = test_data.test_labels.cpu().detach().numpy()\n",
    "print(type(np_test_labels))\n",
    "lunique, lcount = np.unique(np_test_labels, return_counts = True)\n",
    "print(lunique, lcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1c1db8-d1d5-4e4c-8d34-cd655804acb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "type(np_train_labels)=<class 'numpy.ndarray'>\n",
      "type(np_train_data)=<class 'numpy.ndarray'>\n",
      "type(img)=<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np_train_labels[i])\n",
    "img = np_train_data[i]\n",
    "print(f\"{type(np_train_labels)=}\")\n",
    "print(f\"{type(np_train_data)=}\")\n",
    "print(f\"{type(img)=}\")\n",
    "plt.imshow(img, cmap='gray')\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71ddd67f-5028-42ff-bcbb-240e4e064a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(np_train_data)=<class 'numpy.ndarray'>\n",
      "np_train_data.shape=(60000, 28, 28)\n",
      "type(np_train_labels)=<class 'numpy.ndarray'>\n",
      "np_train_labels.shape=(60000,)\n",
      "np.expand_dims(np_train_labels, axis=(1,2)).shape=(60000, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'{type(np_train_data)=}')\n",
    "print(f'{np_train_data.shape=}')\n",
    "print(f'{type(np_train_labels)=}')\n",
    "print(f'{np_train_labels.shape=}')\n",
    "print(f'{np.expand_dims(np_train_labels, axis=(1,2)).shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70303ddd-b33d-4afb-9203-6c9dabeadfbf",
   "metadata": {},
   "source": [
    "## Try function that returns just the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9a3d28-1189-48eb-889c-5a6fdf7f7e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4]\n",
      "50\n",
      "Client 0: [6, 3, 7, 8, 5, 2, 17, 12, 20, 35]\n",
      "Client 1: [19, 10, 18, 16, 13, 11, 24, 29, 37, 46]\n",
      "Client 2: [1, 23, 27, 22, 25, 21, 28, 33, 36, 44]\n",
      "Client 3: [9, 15, 34, 30, 32, 38, 39, 31, 49, 40]\n",
      "Client 4: [0, 4, 14, 26, 47, 41, 45, 42, 43, 48]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 1, 1, 2, 3]),\n",
       " array([1, 1, 1, 1, 1, 1, 2, 2, 3, 4]),\n",
       " array([0, 2, 2, 2, 2, 2, 2, 3, 3, 4]),\n",
       " array([0, 1, 3, 3, 3, 3, 3, 3, 4, 4]),\n",
       " array([0, 0, 1, 2, 4, 4, 4, 4, 4, 4])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test unbal_idx\n",
    "N = 5\n",
    "M = 50\n",
    "art_labels = np.arange(0,N).repeat(M// N)\n",
    "print(art_labels)\n",
    "print(len(art_labels))\n",
    "lengths = [M//N]*N\n",
    "    \n",
    "Ic = unbal_idx(art_labels, N, 0.6)\n",
    "for i, cli in enumerate(Ic):\n",
    "    print(f\"Client {i}: {cli}\")\n",
    "[art_labels[Ii] for Ii in Ic]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66b6bf-bc3e-476e-9ab5-98582a5e8e44",
   "metadata": {},
   "source": [
    "## Try with Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84fc8f89-9a68-41d8-8862-95977055bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 10\n",
    "partition_size = len(training_data) // NUM_CLIENTS\n",
    "lengths = [partition_size] * NUM_CLIENTS\n",
    "# Try with p=0.6 for sum is less, try with p=0.8 for uneven lengths\n",
    "datasets = unbal_split(training_data, lengths, p=0.8, generator=torch.Generator().manual_seed(43))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19604e70-1c04-4e28-8360-485feda692dd",
   "metadata": {},
   "source": [
    "### Visualize partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af653cf3-8d4b-4ac1-ac98-cfb516e5c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: 0 <class 'torch.utils.data.dataset.Subset'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0.8   0.119 0.039 0.018 0.01  0.006 0.004 0.003 0.002 0.001] = 6000\n",
      "\n",
      "Client: 1 <class 'torch.utils.data.dataset.Subset'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0.001 0.8   0.119 0.039 0.018 0.01  0.006 0.004 0.003 0.002] = 6000\n",
      "\n",
      "Client: 2 <class 'torch.utils.data.dataset.Subset'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0.002 0.001 0.8   0.119 0.039 0.018 0.01  0.006 0.004 0.003] = 6000\n",
      "\n",
      "Client: 3 <class 'torch.utils.data.dataset.Subset'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0.003 0.002 0.001 0.8   0.119 0.039 0.018 0.01  0.006 0.004] = 6000\n",
      "\n",
      "Client: 4 <class 'torch.utils.data.dataset.Subset'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0.004 0.003 0.002 0.001 0.8   0.119 0.039 0.018 0.01  0.006] = 6000\n",
      "\n",
      "Client: 5 <class 'torch.utils.data.dataset.Subset'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0.006 0.004 0.003 0.002 0.001 0.8   0.119 0.039 0.018 0.01 ] = 6000\n",
      "\n",
      "Client: 6 <class 'torch.utils.data.dataset.Subset'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0.01  0.006 0.004 0.003 0.002 0.001 0.8   0.119 0.039 0.018] = 6000\n",
      "\n",
      "Client: 7 <class 'torch.utils.data.dataset.Subset'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0.018 0.01  0.006 0.004 0.003 0.002 0.001 0.8   0.119 0.039] = 6000\n",
      "\n",
      "Client: 8 <class 'torch.utils.data.dataset.Subset'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0.039 0.018 0.01  0.006 0.004 0.003 0.002 0.001 0.8   0.119] = 6000\n",
      "\n",
      "Client: 9 <class 'torch.utils.data.dataset.Subset'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0.119 0.039 0.018 0.01  0.006 0.004 0.003 0.002 0.001 0.8  ] = 6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_fashion = {0: \"T-shirt/top\",\n",
    "                1: \"Trouser\",\n",
    "                2: \"Pullover\",\n",
    "                3: \"Dress\",\n",
    "                4: \"Coat\",\n",
    "                5: \"Sandal\",\n",
    "                6: \"Shirt\",\n",
    "                7: \"Sneaker\",\n",
    "                8: \"Bag\",\n",
    "                9: \"Ankle boot\",\n",
    "               }\n",
    "\n",
    "for i, ds in enumerate(datasets):\n",
    "    print(\"Client:\", i, type(ds))\n",
    "    #print(dir(ds))\n",
    "    #print(dir(ds.dataset))\n",
    "    #print(dir(ds.indices))\n",
    "    \n",
    "    np_client_labels = ds.dataset.train_labels[ds.indices].cpu().detach().numpy()\n",
    "    np_client_data   = ds.dataset.train_data[ds.indices].cpu().detach().numpy()\n",
    "    lunique, lcounts = np.unique(np_client_labels, return_counts = True)\n",
    "    lcountsn = np.divide(lcounts, lcounts.sum())\n",
    "    print(lunique)\n",
    "    print(lcountsn.round(3), \"=\", lcounts.sum())\n",
    "    print()\n",
    "    for j in np.linspace(0, partition_size-1, num=10).astype(int):\n",
    "        print(np_client_labels[j], \":\", dict_fashion[np_client_labels[j]])\n",
    "        plt.imshow(np_client_data[j])\n",
    "        plt.show()\n",
    "        \n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe087a0-3918-4cdd-b452-990ac13542bd",
   "metadata": {},
   "source": [
    "## Jihoon's code, here to test if it runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84270752-f317-4d2f-bf40-544cf757b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def load_datasets():\n",
    "    # Define the transformation to Fashion MNIST\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "    \n",
    "    # Load in the Fashion MNIST dataset\n",
    "    training_data = FashionMNIST(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    \n",
    "    test_data = FashionMNIST(\n",
    "        root=root,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Split the training data into NUM_CLIENTS clients\n",
    "    partition_size = len(training_data) // NUM_CLIENTS\n",
    "    lengths = [partition_size] * NUM_CLIENTS\n",
    "    datasets = unbal_split(training_data, lengths, p=0.6, generator=torch.Generator().manual_seed(42))\n",
    "    #datasets = random_split(training_data, lengths, generator=torch.Generator().manual_seed(42))\n",
    "    #print(datasets)\n",
    "    \n",
    "    # Split each partition into train.val and create DataLoader\n",
    "    train_loaders = []\n",
    "    val_loaders = []\n",
    "    for i, ds in enumerate(datasets):\n",
    "        print(i)\n",
    "        length_val = len(ds) // 10 # 10% of the partition is used for validation\n",
    "        length_train = len(ds) - length_val\n",
    "        lengths = [length_train, length_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, generator=torch.Generator().manual_seed(42))\n",
    "        train_loaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
    "        val_loaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
    "    \n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    return train_loaders, val_loaders, test_loader\n",
    "\n",
    "train_loaders, val_loaders, test_loader = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51c5fd3d-aa62-46b4-a0d3-cb1329008ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[5439 5418 5401 5398 5388 5403 5384 5389 5386 5394]  =  54000\n",
      "<class 'list'>\n",
      "<class 'torch.utils.data.dataset.Subset'>\n"
     ]
    }
   ],
   "source": [
    "glabels = []\n",
    "for loaders in [train_loaders]: #, val_loaders]:\n",
    "    for loader in loaders:\n",
    "        for images, labels in loader:\n",
    "            for l in labels:\n",
    "                glabels.append(l.item())\n",
    "            \n",
    "\n",
    "print(glabels[:2])\n",
    "nlabels = np.array(glabels)\n",
    "labels, lcount = np.unique(nlabels, return_counts = True)\n",
    "print(labels)\n",
    "print(lcount, \" = \", lcount.sum())\n",
    "print(type(datasets))\n",
    "print(type(datasets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3e941-6246-4f7b-b9cf-2f7a39f1bde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flower 2022-11-08 13:57:24,456 | app.py:140 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
      "2022-11-08 13:57:24,739\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "INFO flower 2022-11-08 13:57:26,018 | app.py:174 | Flower VCE: Ray initialized with resources: {'object_store_memory': 3120210739.0, 'CPU': 8.0, 'node:192.168.0.12': 1.0, 'memory': 6240421479.0}\n",
      "INFO flower 2022-11-08 13:57:26,019 | server.py:86 | Initializing global parameters\n",
      "INFO flower 2022-11-08 13:57:26,020 | server.py:270 | Requesting initial parameters from one random client\n",
      "INFO flower 2022-11-08 13:57:27,216 | server.py:274 | Received initial parameters from one random client\n",
      "INFO flower 2022-11-08 13:57:27,216 | server.py:88 | Evaluating initial parameters\n",
      "INFO flower 2022-11-08 13:57:27,217 | server.py:101 | FL starting\n",
      "DEBUG flower 2022-11-08 13:57:27,217 | server.py:215 | fit_round 1: strategy sampled 10 clients (out of 10)\n",
      "DEBUG flower 2022-11-08 14:12:34,048 | server.py:229 | fit_round 1 received 10 results and 0 failures\n",
      "WARNING flower 2022-11-08 14:12:34,364 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flower 2022-11-08 14:12:34,367 | server.py:165 | evaluate_round 1: strategy sampled 5 clients (out of 10)\n",
      "DEBUG flower 2022-11-08 14:12:37,049 | server.py:179 | evaluate_round 1 received 5 results and 0 failures\n",
      "DEBUG flower 2022-11-08 14:12:37,050 | server.py:215 | fit_round 2: strategy sampled 10 clients (out of 10)\n"
     ]
    }
   ],
   "source": [
    "def train(net, train_loader, epochs: int, verbose=False):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss \n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")   \n",
    "\n",
    "def test(net, test_loader):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        loss /= len(test_loader.dataset)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "def modelA():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1,64,(5,5), padding='valid'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64,64,(5,5)),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(0.25),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(25600,128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128,10),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def modelB():\n",
    "    model = nn.Sequential(\n",
    "        nn.Dropout2d(0.2),\n",
    "        nn.Conv2d(1,64,(8,8), padding=(3,3), stride=(2,2)),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64,128,(6,6), padding='valid', stride=(2,2)),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(128,128,(5,5), stride=(1,1)),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(0.5),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(128,10),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def modelC():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1,128,(3,3), padding='valid'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(128,64,(3,3)),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(0.25),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(36864,128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128,10)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def modelD():\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(784, 300),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(300, 300),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(300, 300),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(300, 300),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(300, 10),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelE():\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(784, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 10),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelF():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, (5,5,), padding='valid'),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Conv2d(32, 64, (5,5)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 10),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelG():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, (5,5), padding='same'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, (5,5), padding='same'),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Dropout2d(0.25),\n",
    "        nn.Conv2d(32, 64, (3,3), padding='same'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, (3,3), padding='same'),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2), stride=(2,2)),\n",
    "        nn.Dropout2d(0.25),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(3136, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 10)\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def ModelLR():\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(784, 10),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    \"\"\"Get model parameters as a list of NumPy ndarrays.\"\"\"\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]) -> None:\n",
    "    \"\"\"Set model parameters from a list of NumPy ndarrays.\"\"\"\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, train_loader, val_loader):\n",
    "        self.net = net \n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return get_parameters(self.net)\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.train_loader, epochs=10)\n",
    "        return get_parameters(self.net), len(self.train_loader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.val_loader)\n",
    "        return float(loss), len(self.val_loader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "def client_fn(cid: str) -> FlowerClient:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = modelA().to(DEVICE)\n",
    "    return FlowerClient(net, train_loaders[int(cid)], val_loaders[int(cid)])\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "stragegy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "    fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "    min_fit_clients=10,  # Never sample less than 10 clients for training\n",
    "    min_evaluate_clients=5,  # Never sample less than 5 clients for evaluation\n",
    "    min_available_clients=10,  # Wait until all 10 clients are available\n",
    "    evaluate_metrics_aggregation_fn=weighted_average,  # Custom aggregation function\n",
    ")\n",
    "\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=10),\n",
    "    strategy=stragegy\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623b500-19bd-44dc-8618-5c6c7c5dc500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_marlvr",
   "language": "python",
   "name": "env_marlvr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
